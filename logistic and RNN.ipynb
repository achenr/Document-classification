{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\", UserWarning)\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None\n",
    "import numpy as np \n",
    "from string import punctuation\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, auc, roc_auc_score\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "import scipy\n",
    "from scipy.sparse import hstack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     label summary\n",
      "0      NaN     NaN\n",
      "1      NaN     NaN\n",
      "2      NaN     NaN\n",
      "3      NaN     NaN\n",
      "4      NaN     NaN\n",
      "5      NaN     NaN\n",
      "6      NaN     NaN\n",
      "7      NaN     NaN\n",
      "8      NaN     NaN\n",
      "9      NaN     NaN\n",
      "10     NaN     NaN\n",
      "11     NaN     NaN\n",
      "12     NaN     NaN\n",
      "13     NaN     NaN\n",
      "14     NaN     NaN\n",
      "15     NaN     NaN\n",
      "16     NaN     NaN\n",
      "17     NaN     NaN\n",
      "18     NaN     NaN\n",
      "19     NaN     NaN\n",
      "20     NaN     NaN\n",
      "21     NaN     NaN\n",
      "22     NaN     NaN\n",
      "23     NaN     NaN\n",
      "24     NaN     NaN\n",
      "25     NaN     NaN\n",
      "26     NaN     NaN\n",
      "27     NaN     NaN\n",
      "28     NaN     NaN\n",
      "29     NaN     NaN\n",
      "...    ...     ...\n",
      "1583   NaN     NaN\n",
      "1584   NaN     NaN\n",
      "1585   NaN     NaN\n",
      "1586   NaN     NaN\n",
      "1587   NaN     NaN\n",
      "1588   NaN     NaN\n",
      "1589   NaN     NaN\n",
      "1590   NaN     NaN\n",
      "1591   NaN     NaN\n",
      "1592   NaN     NaN\n",
      "1593   NaN     NaN\n",
      "1594   NaN     NaN\n",
      "1595   NaN     NaN\n",
      "1596   NaN     NaN\n",
      "1597   NaN     NaN\n",
      "1598   NaN     NaN\n",
      "1599   NaN     NaN\n",
      "1600   NaN     NaN\n",
      "1601   NaN     NaN\n",
      "1602   NaN     NaN\n",
      "1603   NaN     NaN\n",
      "1604   NaN     NaN\n",
      "1605   NaN     NaN\n",
      "1606   NaN     NaN\n",
      "1607   NaN     NaN\n",
      "1608   NaN     NaN\n",
      "1609   NaN     NaN\n",
      "1610   NaN     NaN\n",
      "1611   NaN     NaN\n",
      "1612   NaN     NaN\n",
      "\n",
      "[1613 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xlrd\n",
    "xls = pd.ExcelFile(r\"C:\\Users\\ABHI\\Desktop\\acads\\sem 3\\capstone\\project\\Prospects 2018.xlsx\")\n",
    "summary = pd.read_excel(xls, 'Sheet1')\n",
    "summary = summary.iloc[:,2:4]\n",
    "summary = summary.replace(np.nan, 'n', regex=True)\n",
    "summary.describe()\n",
    "summary.columns = ['label', 'summary']\n",
    "#print(summary)\n",
    "summary_cleaned = pd.DataFrame(columns=summary.columns,index=summary.index)\n",
    "print(summary_cleaned)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.cluster import KMeans\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import string\n",
    "import re\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    " \n",
    "stop = set(stopwords.words('english'))\n",
    "exclude = set(string.punctuation)\n",
    "lemma = WordNetLemmatizer()\n",
    " \n",
    "# Cleaning the text sentences so that punctuation marks, stop words & digits are removed\n",
    "def clean(doc):\n",
    "    stop_free = \" \".join([i for i in doc.lower().split() if i not in stop])\n",
    "    punc_free = ''.join(ch for ch in stop_free if ch not in exclude)\n",
    "    normalized = \" \".join(lemma.lemmatize(word) for word in punc_free.split())\n",
    "    processed = re.sub(r\"\\d+\",\"\",normalized)\n",
    "    y = processed.split()\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method NDFrame.head of      label                                            summary\n",
      "0        n  ​This Funding Opportunity Announcement (FOA), ...\n",
      "1        n  ​Population-based Research to Optimize the Scr...\n",
      "2        n  ​This Funding Opportunity Announcement (FOA) i...\n",
      "3        y  ​\\nThe purpose of this notice is to announce t...\n",
      "4        n  ​This Funding Opportunity Announcement (FOA) i...\n",
      "5        y  ​\\nThe Washington Health Care Authority (HCA) ...\n",
      "6        n  ​\\nThe National Highway Traffic  Safety Admini...\n",
      "7        y  ​The State of Oregon, acting by and through th...\n",
      "8        n  ​The United States Coast Guard (USCG) Boating ...\n",
      "9        n  ​\\nThe Oakland Fund for Children and Youth (OF...\n",
      "10       y  ​The Massachusetts Department of Early Educati...\n",
      "11       n  ​An academic institution or goverment must be ...\n",
      "12       y  ​The purpose of this Funding Opportunity Annou...\n",
      "13       y  ​The Mayor’s Fund seeks an independent evaluat...\n",
      "14       n  ​\\nThis is a small buisness set aside. The Con...\n",
      "15       n  ​\\nOn June 22, 2009, the Family Smoking Preven...\n",
      "16       n  ​\\nThe Department of State Office to Monitor a...\n",
      "17       n  ​The purpose of this funding opportunity annou...\n",
      "18       y  ​Analysis of the Massachusetts budget by Govwi...\n",
      "19       y  ​The goal of this funding opportunity is to st...\n",
      "20       y  ​\\nThe NSF vision for a Cyberinfrastructure Fr...\n",
      "21       y  ​\\nThis Request for Information (RFI) by The U...\n",
      "22       n  ​The purpose of this funding announcement is t...\n",
      "23       n  ​The Federal Transit Administration (FTA) anno...\n",
      "24       n  ​\\nThe Commission and its collaborating agenci...\n",
      "25       n  The synopsis is anticipated in December 2016. ...\n",
      "26       n  ​The U.S. Department of Education (the Departm...\n",
      "27       n  ​This is a combined synopsis/solicitation for ...\n",
      "28       n  ​The U.S. Department of State, Bureau of Democ...\n",
      "29       n  ​Please note, ORI anticipates issuing the soli...\n",
      "...    ...                                                ...\n",
      "1583     n  \\nThe U.S. Government, represented by the Unit...\n",
      "1584     n  \\nPreviously announced, this FOA has been modi...\n",
      "1585     n  \\nThe goal of this FOA is to provide CBA to OA...\n",
      "1586     n  \\nThe Government of Canada is committed to red...\n",
      "1587     y  \\nThe Defense Media Activity (DMA) is hosting ...\n",
      "1588     n  \\nThe National Oceanic and Atmospheric Adminis...\n",
      "1589     y  \\nThis is a solicitation of the Massachusetts ...\n",
      "1590     n  \\nThe purpose of this RFQ is to obtain a consu...\n",
      "1591     n  \\nPlease note, the link to the RFP presented b...\n",
      "1592     n  \\nThe Data Infrastructure Building Blocks (DIB...\n",
      "1593     n  We are seeking to identify optimal data system...\n",
      "1594     n  \\nSeventy years ago, John von Neumann found in...\n",
      "1595     n  \\nThis was announced previously. The submissio...\n",
      "1596     n  \\nThe Accountable Health Communities model, as...\n",
      "1597     y  \\nThe JPEO-CBD’s mission is to provide Researc...\n",
      "1598     n  \\nThis FOA is a program announcement with mult...\n",
      "1599     n  \\nThis is a small business sources sought noti...\n",
      "1600     n  \\nThis is a small business set aside. \\n \\nThe...\n",
      "1601     y  \\nThe current contract with Logicon expires in...\n",
      "1602     n  \\nThis is a small business sources sought noti...\n",
      "1603     n  \\nPurpose. To acquire contractor support to as...\n",
      "1604     n  \\nThe purpose of this solicitation is to estab...\n",
      "1605     n  \\nThis is a small business sources sought noti...\n",
      "1606     n  \\nThe purpose of this Request for Proposal (RF...\n",
      "1607     n  \\nThis is a small business sources sought noti...\n",
      "1608     n  \\nThe United States Agency for International D...\n",
      "1609     n  \\nThis is a small business sources sought noti...\n",
      "1610     n  This is a SOURCES SOUGHT NOTICE issued by the ...\n",
      "1611     n  \\nThis procurement is on the Fedconnect portal...\n",
      "1612     n  \\nThe purpose of the Cambodia Commercial Horti...\n",
      "\n",
      "[1613 rows x 2 columns]>\n",
      "******\n",
      "     label                                            summary\n",
      "0        n  ​this funding opportunity announcement foa col...\n",
      "1        n  ​populationbased research optimize screening p...\n",
      "2        n  ​this funding opportunity announcement foa inv...\n",
      "3        y  ​ purpose notice announce global research asse...\n",
      "4        n  ​this funding opportunity announcement foa new...\n",
      "5        y  ​ washington health care authority hca request...\n",
      "6        n  ​ national highway traffic safety administrati...\n",
      "7        y  ​the state oregon acting department environmen...\n",
      "8        n  ​the united state coast guard uscg boating saf...\n",
      "9        n  ​ oakland fund child youth ofcy seeking indepe...\n",
      "10       y  ​the massachusetts department early education ...\n",
      "11       n  ​an academic institution goverment must primet...\n",
      "12       y  ​the purpose funding opportunity announcement ...\n",
      "13       y  ​the mayor’s fund seek independent evaluator c...\n",
      "14       n  ​ small buisness set aside contractor shall de...\n",
      "15       n  ​ june family smoking prevention tobacco contr...\n",
      "16       n  ​ department state office monitor combat traff...\n",
      "17       n  ​the purpose funding opportunity announcement ...\n",
      "18       y  ​analysis massachusetts budget govwin indicate...\n",
      "19       y  ​the goal funding opportunity stimulate use ex...\n",
      "20       y  ​ nsf vision cyberinfrastructure framework cen...\n",
      "21       y  ​ request information rfi u department labor s...\n",
      "22       n  ​the purpose funding announcement support coor...\n",
      "23       n  ​the federal transit administration fta announ...\n",
      "24       n  ​ commission collaborating agency seek one qua...\n",
      "25       n  synopsis anticipated december ​ funding opport...\n",
      "26       n  ​the u department education the department con...\n",
      "27       n  ​this combined synopsissolicitation commercial...\n",
      "28       n  ​the u department state bureau democracy human...\n",
      "29       n  ​please note ori anticipates issuing soliciati...\n",
      "...    ...                                                ...\n",
      "1583     n  u government represented united state agency i...\n",
      "1584     n  previously announced foa modified purpose foa ...\n",
      "1585     n  goal foa provide cba oahfunded tpp grantee fiv...\n",
      "1586     n  government canada committed reducing crime enh...\n",
      "1587     y  defense medium activity dma hosting industry d...\n",
      "1588     n  national oceanic atmospheric administration ag...\n",
      "1589     y  solicitation massachusetts department public h...\n",
      "1590     n  purpose rfq obtain consultant assist maryland ...\n",
      "1591     n  please note link rfp presented working earlier...\n",
      "1592     n  data infrastructure building block dibbs progr...\n",
      "1593     n  seeking identify optimal data system collectin...\n",
      "1594     n  seventy year ago john von neumann found inspir...\n",
      "1595     n  announced previously submission date changed f...\n",
      "1596     n  accountable health community model authorized ...\n",
      "1597     y  jpeocbd’s mission provide research development...\n",
      "1598     n  foa program announcement multiple receipt date...\n",
      "1599     n  small business source sought notice national h...\n",
      "1600     n  small business set aside united state departme...\n",
      "1601     y  current contract logicon expires december depa...\n",
      "1602     n  small business source sought notice average ye...\n",
      "1603     n  purpose acquire contractor support assist cm p...\n",
      "1604     n  purpose solicitation establish prequalified “v...\n",
      "1605     n  small business source sought notice nhtsa inte...\n",
      "1606     n  purpose request proposal rfp solicit applicati...\n",
      "1607     n  small business source sought notice overall tr...\n",
      "1608     n  united state agency international development ...\n",
      "1609     n  small business source sought notice bicyclist ...\n",
      "1610     n  source sought notice issued u agency internati...\n",
      "1611     n  procurement fedconnect portal although login n...\n",
      "1612     n  purpose cambodia commercial horticulture value...\n",
      "\n",
      "[1613 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "train_clean_sentences = []\n",
    "for i in range(len(summary_cleaned)):\n",
    "    summary_cleaned.iloc[i,0] = summary.iloc[i,0]\n",
    "    summary_cleaned.iloc[i,1] = (' '.join(clean(summary.iloc[i,1]))).strip()\n",
    "    train_clean_sentences.append(summary_cleaned.iloc[i,1])\n",
    "    \n",
    "    \n",
    "print(summary.head)\n",
    "print('******')\n",
    "print(summary_cleaned)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1451,) (162,) (1451,) (162,)\n"
     ]
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(summary_cleaned['summary'], \n",
    "                                                    summary_cleaned['label'], \n",
    "                                                    test_size=0.1, \n",
    "                                                    random_state=42,\n",
    "                                                    stratify=summary_cleaned['label'])\n",
    "\n",
    "print(x_train.shape, x_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer_word = TfidfVectorizer(max_features=40000,\n",
    "                             min_df=5, \n",
    "                             max_df=0.5, \n",
    "                             analyzer='word', \n",
    "                             stop_words='english', \n",
    "                             ngram_range=(1, 2))\n",
    "\n",
    "vectorizer_word.fit(x_train)\n",
    "\n",
    "tfidf_matrix_word_train = vectorizer_word.transform(x_train)\n",
    "tfidf_matrix_word_test = vectorizer_word.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "convergence after 17 epochs took 0 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=None, solver='sag',\n",
       "          tol=0.0001, verbose=2, warm_start=False)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_word = LogisticRegression(solver='sag', verbose=2)\n",
    "lr_word.fit(tfidf_matrix_word_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9012345679012346\n"
     ]
    }
   ],
   "source": [
    "y_pred_word = lr_word.predict(tfidf_matrix_word_test)\n",
    "print(accuracy_score(y_test, y_pred_word))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "convergence after 24 epochs took 2 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    1.7s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    1.7s finished\n"
     ]
    }
   ],
   "source": [
    "vectorizer_char = TfidfVectorizer(max_features=40000,\n",
    "                             min_df=5, \n",
    "                             max_df=0.5, \n",
    "                             analyzer='char', \n",
    "                             ngram_range=(1, 4))\n",
    "\n",
    "vectorizer_char.fit(x_train)\n",
    "\n",
    "tfidf_matrix_char_train = vectorizer_char.transform(x_train)\n",
    "tfidf_matrix_char_test = vectorizer_char.transform(x_test)\n",
    "\n",
    "lr_char = LogisticRegression(solver='sag', verbose=2)\n",
    "lr_char.fit(tfidf_matrix_char_train, y_train)\n",
    "\n",
    "y_pred_char = lr_char.predict(tfidf_matrix_char_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9012345679012346\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(y_test, y_pred_char))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "convergence after 45 epochs took 2 seconds\n",
      "0.9074074074074074\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    2.5s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    2.5s finished\n"
     ]
    }
   ],
   "source": [
    "tfidf_matrix_word_char_train =  hstack((tfidf_matrix_word_train, tfidf_matrix_char_train))\n",
    "tfidf_matrix_word_char_test =  hstack((tfidf_matrix_word_test, tfidf_matrix_char_test))\n",
    "\n",
    "lr_word_char = LogisticRegression(solver='sag', verbose=2)\n",
    "lr_word_char.fit(tfidf_matrix_word_char_train, y_train)\n",
    "\n",
    "y_pred_word_char = lr_word_char.predict(tfidf_matrix_word_char_test)\n",
    "print(accuracy_score(y_test, y_pred_word_char))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.models import Sequential\n",
    "\n",
    "from keras.layers import Input, Dense, Embedding, Conv1D, Conv2D, MaxPooling1D, MaxPool2D\n",
    "from keras.layers import Reshape, Flatten, Dropout, Concatenate\n",
    "from keras.layers import SpatialDropout1D, concatenate\n",
    "from keras.layers import GRU, Bidirectional, GlobalAveragePooling1D, GlobalMaxPooling1D\n",
    "\n",
    "from keras.callbacks import Callback\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.models import load_model\n",
    "from keras.utils.vis_utils import plot_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_NB_WORDS = 80000\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "\n",
    "tokenizer.fit_on_texts(summary_cleaned['summary'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "​ june family smoking prevention tobacco control act fsptca signed law act give authority food drug administration fda enact facilitate tobacco product regulation overall goal prevent americans—especially youth—from starting use tobacco encourage current user quit decrease harm tobacco product use act fda’s center tobacco product ctp created regulate cigarette cigarette tobacco rollyourown tobacco smokeless tobacco fda finalized deeming rule extending fda’s authority include regulation electronic nicotine delivery system such ecigarettes vape pen cigar hookah waterpipe tobacco pipe tobacco nicotine gel among others deeming rule went effect august overarching objective rfi obtain declaration technical capability various information data material qualified business concern specific project objective understand hookah commercial landscape includes competitive set manufacturing distribution hookah equipment smoked product hookah tobacco steam stone distribution channel subset endusers customer eg cultural user collegeage user force trend may affect hookah commerce\n",
      "[[42, 881, 152, 1320, 64, 235, 165, 104, 8007, 2133, 316, 104, 1781, 585, 338, 230, 130, 567, 8008, 391, 235, 175, 575, 378, 90, 615, 8009, 8010, 3139, 30, 235, 861, 101, 334, 4315, 1643, 3448, 235, 175, 30, 104, 3449, 23, 235, 175, 2512, 1021, 3828, 3829, 3829, 235, 8011, 235, 5018, 235, 567, 5019, 5983, 865, 3450, 3449, 585, 38, 575, 410, 5020, 189, 13, 1253, 5984, 8012, 8013, 5985, 3830, 8014, 235, 8015, 235, 5020, 8016, 147, 840, 5983, 865, 8017, 464, 1037, 1644, 84, 354, 397, 4316, 32, 144, 309, 8, 1, 195, 206, 26, 702, 149, 11, 84, 396, 3830, 516, 2675, 238, 762, 141, 2017, 722, 3830, 830, 5986, 175, 3830, 235, 8018, 8019, 722, 2134, 2233, 5987, 549, 187, 778, 334, 8020, 334, 659, 530, 41, 897, 3830, 2372]]\n"
     ]
    }
   ],
   "source": [
    "print(x_train[15])\n",
    "print(tokenizer.texts_to_sequences([x_train[15]]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sequences = tokenizer.texts_to_sequences(x_train)\n",
    "test_sequences = tokenizer.texts_to_sequences(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 100\n",
    "padded_train_sequences = pad_sequences(train_sequences, maxlen=MAX_LENGTH)\n",
    "padded_test_sequences = pad_sequences(test_sequences, maxlen=MAX_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1451, 100)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_train_sequences\n",
    "padded_train_sequences.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_simple_rnn_model():\n",
    "    embedding_dim = 300\n",
    "    embedding_matrix = np.random.random((MAX_NB_WORDS, embedding_dim))\n",
    "    \n",
    "    inp = Input(shape=(MAX_LENGTH, ))\n",
    "    x = Embedding(input_dim=MAX_NB_WORDS, output_dim=embedding_dim, input_length=MAX_LENGTH, \n",
    "                  weights=[embedding_matrix], trainable=True)(inp)\n",
    "    x = SpatialDropout1D(0.3)(x)\n",
    "    x = Bidirectional(GRU(100, return_sequences=True))(x)\n",
    "    avg_pool = GlobalAveragePooling1D()(x)\n",
    "    max_pool = GlobalMaxPooling1D()(x)\n",
    "    conc = concatenate([avg_pool, max_pool])\n",
    "    outp = Dense(1, activation=\"sigmoid\")(conc)\n",
    "    \n",
    "    model = Model(inputs=inp, outputs=outp)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "rnn_simple_model = get_simple_rnn_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Failed to import `pydot`. Please install `pydot`. For example with `pip install pydot`.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-46-0e35af830f7e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m            \u001b[0mto_file\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'rnn_simple_model.png'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m            \u001b[0mshow_shapes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m            show_layer_names=True)\n\u001b[0m",
      "\u001b[1;32m~\\Miniconda3\\lib\\site-packages\\keras\\utils\\vis_utils.py\u001b[0m in \u001b[0;36mplot_model\u001b[1;34m(model, to_file, show_shapes, show_layer_names, rankdir)\u001b[0m\n\u001b[0;32m    130\u001b[0m             \u001b[1;34m'LR'\u001b[0m \u001b[0mcreates\u001b[0m \u001b[0ma\u001b[0m \u001b[0mhorizontal\u001b[0m \u001b[0mplot\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    131\u001b[0m     \"\"\"\n\u001b[1;32m--> 132\u001b[1;33m     \u001b[0mdot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel_to_dot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshow_shapes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshow_layer_names\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrankdir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    133\u001b[0m     \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mextension\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplitext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mto_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    134\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mextension\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\lib\\site-packages\\keras\\utils\\vis_utils.py\u001b[0m in \u001b[0;36mmodel_to_dot\u001b[1;34m(model, show_shapes, show_layer_names, rankdir)\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 55\u001b[1;33m     \u001b[0m_check_pydot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     56\u001b[0m     \u001b[0mdot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpydot\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m     \u001b[0mdot\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'rankdir'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrankdir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\lib\\site-packages\\keras\\utils\\vis_utils.py\u001b[0m in \u001b[0;36m_check_pydot\u001b[1;34m()\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mpydot\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m         raise ImportError(\n\u001b[1;32m---> 20\u001b[1;33m             \u001b[1;34m'Failed to import `pydot`. '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m             \u001b[1;34m'Please install `pydot`. '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m             'For example with `pip install pydot`.')\n",
      "\u001b[1;31mImportError\u001b[0m: Failed to import `pydot`. Please install `pydot`. For example with `pip install pydot`."
     ]
    }
   ],
   "source": [
    "import pydot\n",
    "from keras.utils.vis_utils import plot_model\n",
    "\n",
    "plot_model(rnn_simple_model, \n",
    "           to_file='rnn_simple_model.png', \n",
    "           show_shapes=True, \n",
    "           show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tuple'>\n",
      "[0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 1 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0\n",
      " 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 1 0 1 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "print(type(y_train))\n",
    "#y_train = pd.factorize(y_train)\n",
    "#y_test = pd.factorize(y_test)\n",
    "print (np.array(y_test[0]))\n",
    "y_train = np.array(y_train[0])\n",
    "y_test = np.array(y_test[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1451 samples, validate on 162 samples\n",
      "Epoch 1/2\n",
      "1451/1451 [==============================] - 25s 17ms/step - loss: 0.2973 - acc: 0.9035 - val_loss: 0.2963 - val_acc: 0.9012\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.90123, saving model to weights-improvement-01-0.9012.hdf5\n",
      "Epoch 2/2\n",
      "1451/1451 [==============================] - 24s 17ms/step - loss: 0.2904 - acc: 0.9035 - val_loss: 0.2986 - val_acc: 0.9012\n",
      "\n",
      "Epoch 00002: val_acc did not improve from 0.90123\n",
      "162/162 [==============================] - 1s 8ms/step\n"
     ]
    }
   ],
   "source": [
    "filepath=\"weights-improvement-{epoch:02d}-{val_acc:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "\n",
    "batch_size = 256\n",
    "epochs = 2\n",
    "\n",
    "history = rnn_simple_model.fit(x=padded_train_sequences, \n",
    "                    y=y_train, \n",
    "                    validation_data=(padded_test_sequences, y_test), \n",
    "                    batch_size=batch_size, \n",
    "                    callbacks=[checkpoint], \n",
    "                    epochs=epochs, \n",
    "                    verbose=1)\n",
    "\n",
    "best_rnn_simple_model = load_model('weights-improvement-01-0.9012.hdf5')\n",
    "\n",
    "y_pred_rnn_simple = best_rnn_simple_model.predict(padded_test_sequences, verbose=1, batch_size=2048)\n",
    "\n",
    "y_pred_rnn_simple = pd.DataFrame(y_pred_rnn_simple, columns=['prediction'])\n",
    "y_pred_rnn_simple['prediction'] = y_pred_rnn_simple['prediction'].map(lambda p: 1 if p >= 0.5 else 0)\n",
    "y_pred_rnn_simple.to_csv('y_pred_rnn_simple.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9012345679012346\n"
     ]
    }
   ],
   "source": [
    "y_pred_rnn_simple = pd.read_csv('y_pred_rnn_simple.csv')\n",
    "print(accuracy_score(y_test, y_pred_rnn_simple))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
